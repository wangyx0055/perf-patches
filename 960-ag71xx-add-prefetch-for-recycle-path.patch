--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -492,9 +492,15 @@ struct sk_buff *__netdev_alloc_skb(struc
 		local_irq_restore(flags);
 		put_cpu_var(recycle_list);
 		if (likely(skb)) {
-			struct skb_shared_info *s = skb_shinfo(skb);
-			zero_struct(s, offsetof(struct skb_shared_info, dataref));
-			atomic_set(&s->dataref, 1);
+			struct skb_shared_info *shinfo;
+
+			/*
+			 * We're about to write a large amount to the skb to
+			 * zero most of the structure so prefetch the start
+			 * of the shinfo region now so it's in the D-cache
+			 * before we start to write that.
+			 */
+			prefetchw(&skb->end);
 
 			zero_struct(skb, offsetof(struct sk_buff, tail));
 			skb->data = skb->head;
@@ -503,6 +509,10 @@ struct sk_buff *__netdev_alloc_skb(struc
 			skb->mac_header = (typeof(skb->mac_header))~0U;
 			skb->transport_header = (typeof(skb->transport_header))~0U;
 
+			shinfo = skb_shinfo(skb);
+			zero_struct(shinfo, offsetof(struct skb_shared_info, dataref));
+			atomic_set(&shinfo->dataref, 1);
+
 			skb_reserve(skb, NET_SKB_PAD);
 			skb->dev = dev;
 			return skb;
@@ -803,6 +813,9 @@ void consume_skb(struct sk_buff *skb)
 {
 	if (unlikely(!skb))
 		return;
+
+	prefetch(&skb->destructor);
+
 	if (likely(atomic_read(&skb->users) == 1))
 		smp_rmb();
 	else if (likely(!atomic_dec_and_test(&skb->users)))
